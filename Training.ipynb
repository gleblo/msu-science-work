{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "mount_file_id": "1x0awCivVGFAPzYiAj71vbKdsSVLL9kJ3",
      "authorship_tag": "ABX9TyMABbNX6PTYHVio/QM+4RP1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY7ScQHQIC9z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c0d720fc-fbda-450a-d09b-14f9dbcea165"
      },
      "source": [
        "import os\n",
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmyQQ7BkWi83"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSOQdYxrIPft"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDHl8oB1IRo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "f94718e8-c5bc-40a5-d7a8-e2e9524a918c"
      },
      "source": [
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 9344941547171870114\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 6627143861116710704\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 3253136720051650699\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15695549568\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 13624947861441220974\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sjg8izcguEX"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w2nGxYvhNYP"
      },
      "source": [
        "Required functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGJwv1LPIloA"
      },
      "source": [
        "\n",
        "char_list = list('-0123456789')\n",
        "\n",
        " \n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua2arHVNhGkN"
      },
      "source": [
        "Loading labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6vLWlSzDbU1"
      },
      "source": [
        "import pandas as pd\n",
        "paths = pd.read_csv('/content/drive/My Drive/15k toloka/checked_images.csv')#,keep_default_na=False)\n",
        "paths = paths.dropna()\n",
        "# paths.fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkgeT1VWg_YN"
      },
      "source": [
        "Number of training samples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1VtW_8fyqXF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e82243da-2ac7-4d34-a886-c9fb7dcb9995"
      },
      "source": [
        "len(paths.index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6825"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wS0If8fhT8i"
      },
      "source": [
        "We can load data from images or from npz file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpifQJfyIqJ5"
      },
      "source": [
        "path = '/content/drive/My Drive/15k toloka/images/data/'\n",
        " \n",
        " \n",
        "# lists for training dataset\n",
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        " \n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        " \n",
        "max_label_len = 0\n",
        "read_again = False\n",
        "\n",
        "\n",
        "if read_again == False:\n",
        "    files = np.load(\"./drive/My Drive/15k toloka/dat.npz\",allow_pickle=True)\n",
        "\n",
        "    valid_orig_txt = files[\"arr_0\"]\n",
        "    valid_label_length = files[\"arr_1\"]\n",
        "    valid_input_length = files[\"arr_2\"]\n",
        "    valid_img = files[\"arr_3\"]\n",
        "    valid_txt = files[\"arr_4\"]\n",
        "    orig_txt = files[\"arr_5\"]\n",
        "    train_label_length = files[\"arr_6\"]\n",
        "    train_input_length = files[\"arr_7\"]\n",
        "    training_img = files[\"arr_8\"]\n",
        "    training_txt = files[\"arr_9\"]\n",
        "    max_label_len = max(valid_label_length.max() , train_label_length.max())\n",
        "\n",
        "else:\n",
        "    i =1 \n",
        "    flag = 0\n",
        "\n",
        "    n_samples = 6825\n",
        "    out = display(progress(0, n_samples-1), display_id=True)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    for index, row in paths.iterrows():\n",
        "        out.update(progress(i, n_samples))\n",
        "        # print(f_name, flush = True)\n",
        "        # read input image and convert into gray scale image\n",
        "        f_name = row[\"file\"]\n",
        "        img = cv2.cvtColor(cv2.imread(os.path.join(path, f_name)), cv2.COLOR_BGR2GRAY)   \n",
        "        # convert each image of shape (32, 128, 1)\n",
        "        w, h = img.shape\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        img = cv2.resize(img, dsize=(128, 32), interpolation=cv2.INTER_CUBIC)\n",
        "        img = np.expand_dims(img , axis = 2)\n",
        "\n",
        "        # Normalize each image\n",
        "        img = img/255.\n",
        "        \n",
        "        # get the text from the image\n",
        "        txt = str(int(row[\"label\"]))\n",
        "        # print(f_name.split('_'), flush = True)\n",
        "        # compute maximum length of the text\n",
        "        # print(txt)\n",
        "        if len(txt) > max_label_len:\n",
        "            max_label_len = len(txt)\n",
        "            \n",
        "            \n",
        "        # split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
        "        if i%5 == 0:     \n",
        "            valid_orig_txt.append(txt)   \n",
        "            valid_label_length.append(len(txt))\n",
        "            valid_input_length.append(31)\n",
        "            valid_img.append(img)\n",
        "            valid_txt.append(encode_to_labels(txt))\n",
        "        else:\n",
        "            orig_txt.append(txt)   \n",
        "            train_label_length.append(len(txt))\n",
        "            train_input_length.append(31)\n",
        "            training_img.append(img)\n",
        "            training_txt.append(encode_to_labels(txt)) \n",
        "        \n",
        "        # break the loop if total data is 150000\n",
        "        if i == n_samples:\n",
        "            flag = 1\n",
        "            break\n",
        "        i+=1\n",
        "        if flag == 1:\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eH4mW0yIsVG"
      },
      "source": [
        "# pad each output label to maximum text length\n",
        " \n",
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jMLhSEahgSM"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbJX4fWBIunj"
      },
      "source": [
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        "batch_norm_1 = BatchNormalization()(pool_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(batch_norm_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        "batch_norm_2 = BatchNormalization()(pool_2)\n",
        "\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(batch_norm_2)\n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        "batch_norm_3 = BatchNormalization()(pool_4)\n",
        "\n",
        "\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_3)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J-Iu8dOIwfP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "abcef760-e54a-472b-cbf3-d799c7f65f7c"
      },
      "source": [
        "act_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 128, 1)]      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 128, 64)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 16, 64, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 32, 128)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 32, 128)        512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 32, 256)        295168    \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 32, 256)        590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 32, 256)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 32, 256)        1024      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 32, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 32, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 32, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 1, 31, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 31, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 31, 256)           656384    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 31, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 31, 12)            3084      \n",
            "=================================================================\n",
            "Total params: 6,608,396\n",
            "Trainable params: 6,605,452\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQrBpYrIzuf"
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "# input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "input_length = Input(name='input_length', shape=[1])\n",
        "label_length = Input(name='label_length', shape=[1])\n",
        "# label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpdLWPp1I1gQ"
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "filepath=\"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdrGnV3hI3HY"
      },
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length).astype(float)\n",
        "train_label_length = np.array(train_label_length).astype(float)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnBnuiDQhnBI"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHznQy4eI5i-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3c53398-466e-4bea-9b9a-6c78327c0675"
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 500\n",
        "history = model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5460 samples, validate on 1365 samples\n",
            "Epoch 1/500\n",
            "5460/5460 [==============================] - 19s 3ms/step - loss: 8.4254 - val_loss: 14.3094\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 14.30942, saving model to best_model.hdf5\n",
            "Epoch 2/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 3.5018 - val_loss: 24.3145\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 14.30942\n",
            "Epoch 3/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.9864 - val_loss: 27.0605\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 14.30942\n",
            "Epoch 4/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.7188 - val_loss: 25.5702\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 14.30942\n",
            "Epoch 5/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.6074 - val_loss: 28.6193\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 14.30942\n",
            "Epoch 6/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.5315 - val_loss: 23.3049\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 14.30942\n",
            "Epoch 7/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.5026 - val_loss: 30.9487\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 14.30942\n",
            "Epoch 8/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.5066 - val_loss: 24.8347\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 14.30942\n",
            "Epoch 9/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.4650 - val_loss: 21.1602\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 14.30942\n",
            "Epoch 10/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.4508 - val_loss: 2.6027\n",
            "\n",
            "Epoch 00010: val_loss improved from 14.30942 to 2.60271, saving model to best_model.hdf5\n",
            "Epoch 11/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.4570 - val_loss: 4.6863\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.60271\n",
            "Epoch 12/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.4240 - val_loss: 0.7625\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.60271 to 0.76252, saving model to best_model.hdf5\n",
            "Epoch 13/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.4034 - val_loss: 5.4387\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.76252\n",
            "Epoch 14/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3974 - val_loss: 8.1366\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.76252\n",
            "Epoch 15/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3771 - val_loss: 3.7704\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.76252\n",
            "Epoch 16/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3435 - val_loss: 17.9527\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.76252\n",
            "Epoch 17/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3586 - val_loss: 8.1557\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.76252\n",
            "Epoch 18/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3396 - val_loss: 1.0858\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.76252\n",
            "Epoch 19/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3648 - val_loss: 11.0491\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.76252\n",
            "Epoch 20/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3602 - val_loss: 5.4327\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.76252\n",
            "Epoch 21/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3522 - val_loss: 15.7237\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.76252\n",
            "Epoch 22/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3733 - val_loss: 8.3984\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.76252\n",
            "Epoch 23/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3308 - val_loss: 4.1265\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.76252\n",
            "Epoch 24/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3247 - val_loss: 4.0058\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.76252\n",
            "Epoch 25/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2958 - val_loss: 2.1729\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.76252\n",
            "Epoch 26/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.3087 - val_loss: 14.6421\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.76252\n",
            "Epoch 27/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3146 - val_loss: 19.9199\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.76252\n",
            "Epoch 28/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3110 - val_loss: 8.9486\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.76252\n",
            "Epoch 29/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2707 - val_loss: 0.7875\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.76252\n",
            "Epoch 30/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2703 - val_loss: 5.5886\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.76252\n",
            "Epoch 31/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.3076 - val_loss: 2.9830\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.76252\n",
            "Epoch 32/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.2906 - val_loss: 0.8966\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.76252\n",
            "Epoch 33/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2505 - val_loss: 0.9426\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.76252\n",
            "Epoch 34/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2403 - val_loss: 9.0847\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.76252\n",
            "Epoch 35/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2569 - val_loss: 0.8659\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.76252\n",
            "Epoch 36/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2567 - val_loss: 1.9989\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.76252\n",
            "Epoch 37/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2432 - val_loss: 8.0020\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.76252\n",
            "Epoch 38/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2309 - val_loss: 0.9408\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.76252\n",
            "Epoch 39/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2072 - val_loss: 3.2745\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.76252\n",
            "Epoch 40/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2155 - val_loss: 1.4824\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.76252\n",
            "Epoch 41/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2603 - val_loss: 11.4601\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.76252\n",
            "Epoch 42/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2464 - val_loss: 1.6741\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.76252\n",
            "Epoch 43/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2184 - val_loss: 2.2572\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.76252\n",
            "Epoch 44/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1931 - val_loss: 3.5344\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.76252\n",
            "Epoch 45/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2106 - val_loss: 0.8251\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.76252\n",
            "Epoch 46/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2123 - val_loss: 0.7339\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.76252 to 0.73387, saving model to best_model.hdf5\n",
            "Epoch 47/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.2100 - val_loss: 2.2184\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.73387\n",
            "Epoch 48/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2167 - val_loss: 2.1377\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.73387\n",
            "Epoch 49/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1863 - val_loss: 3.1593\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.73387\n",
            "Epoch 50/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1909 - val_loss: 8.4255\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.73387\n",
            "Epoch 51/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1918 - val_loss: 0.8474\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.73387\n",
            "Epoch 52/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2015 - val_loss: 8.9822\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.73387\n",
            "Epoch 53/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1995 - val_loss: 6.0919\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.73387\n",
            "Epoch 54/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1657 - val_loss: 8.5799\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.73387\n",
            "Epoch 55/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2324 - val_loss: 7.9374\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.73387\n",
            "Epoch 56/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2195 - val_loss: 16.5805\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.73387\n",
            "Epoch 57/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.2024 - val_loss: 4.8868\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.73387\n",
            "Epoch 58/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1885 - val_loss: 3.5824\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.73387\n",
            "Epoch 59/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1514 - val_loss: 2.0396\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.73387\n",
            "Epoch 60/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1257 - val_loss: 1.6053\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.73387\n",
            "Epoch 61/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1306 - val_loss: 1.0789\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.73387\n",
            "Epoch 62/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1516 - val_loss: 1.8535\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.73387\n",
            "Epoch 63/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1285 - val_loss: 0.9618\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.73387\n",
            "Epoch 64/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1174 - val_loss: 1.6348\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.73387\n",
            "Epoch 65/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1452 - val_loss: 3.1950\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.73387\n",
            "Epoch 66/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1523 - val_loss: 1.4267\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.73387\n",
            "Epoch 67/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1624 - val_loss: 0.8657\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.73387\n",
            "Epoch 68/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1364 - val_loss: 5.9410\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.73387\n",
            "Epoch 69/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1459 - val_loss: 34.9410\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.73387\n",
            "Epoch 70/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1519 - val_loss: 6.7750\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.73387\n",
            "Epoch 71/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1628 - val_loss: 1.7676\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.73387\n",
            "Epoch 72/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1689 - val_loss: 2.1198\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.73387\n",
            "Epoch 73/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1990 - val_loss: 2.5582\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.73387\n",
            "Epoch 74/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1348 - val_loss: 1.3601\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.73387\n",
            "Epoch 75/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1279 - val_loss: 0.9433\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.73387\n",
            "Epoch 76/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1112 - val_loss: 1.8905\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.73387\n",
            "Epoch 77/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1146 - val_loss: 1.2208\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.73387\n",
            "Epoch 78/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.1758 - val_loss: 8.4768\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.73387\n",
            "Epoch 79/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1405 - val_loss: 0.8316\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.73387\n",
            "Epoch 80/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1135 - val_loss: 0.7890\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.73387\n",
            "Epoch 81/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0959 - val_loss: 1.0836\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.73387\n",
            "Epoch 82/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0949 - val_loss: 0.8587\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.73387\n",
            "Epoch 83/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0827 - val_loss: 0.8322\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.73387\n",
            "Epoch 84/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1058 - val_loss: 1.1161\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.73387\n",
            "Epoch 85/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0921 - val_loss: 0.9518\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.73387\n",
            "Epoch 86/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0885 - val_loss: 4.4368\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.73387\n",
            "Epoch 87/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0868 - val_loss: 1.1993\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.73387\n",
            "Epoch 88/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0729 - val_loss: 3.7471\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.73387\n",
            "Epoch 89/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0770 - val_loss: 10.6099\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.73387\n",
            "Epoch 90/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0746 - val_loss: 1.0869\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.73387\n",
            "Epoch 91/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0770 - val_loss: 1.0310\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.73387\n",
            "Epoch 92/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0778 - val_loss: 0.9797\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.73387\n",
            "Epoch 93/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0678 - val_loss: 0.8971\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.73387\n",
            "Epoch 94/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0599 - val_loss: 1.1955\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.73387\n",
            "Epoch 95/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0620 - val_loss: 0.9277\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.73387\n",
            "Epoch 96/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0739 - val_loss: 1.5510\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.73387\n",
            "Epoch 97/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0724 - val_loss: 4.8810\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.73387\n",
            "Epoch 98/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0842 - val_loss: 10.2109\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.73387\n",
            "Epoch 99/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1251 - val_loss: 5.6616\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.73387\n",
            "Epoch 100/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.2019 - val_loss: 1.3798\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.73387\n",
            "Epoch 101/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1503 - val_loss: 2.4910\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.73387\n",
            "Epoch 102/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1031 - val_loss: 2.5535\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.73387\n",
            "Epoch 103/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0907 - val_loss: 1.1840\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.73387\n",
            "Epoch 104/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0734 - val_loss: 1.8126\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.73387\n",
            "Epoch 105/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0618 - val_loss: 3.2377\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.73387\n",
            "Epoch 106/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0583 - val_loss: 0.9809\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.73387\n",
            "Epoch 107/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0434 - val_loss: 1.0304\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.73387\n",
            "Epoch 108/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0441 - val_loss: 1.1737\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.73387\n",
            "Epoch 109/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0502 - val_loss: 1.1709\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.73387\n",
            "Epoch 110/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0509 - val_loss: 6.1041\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.73387\n",
            "Epoch 111/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0768 - val_loss: 2.0060\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.73387\n",
            "Epoch 112/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0640 - val_loss: 1.1557\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.73387\n",
            "Epoch 113/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0593 - val_loss: 2.3203\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.73387\n",
            "Epoch 114/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0587 - val_loss: 5.9078\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.73387\n",
            "Epoch 115/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0764 - val_loss: 1.4679\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.73387\n",
            "Epoch 116/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0906 - val_loss: 0.9630\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.73387\n",
            "Epoch 117/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0946 - val_loss: 1.2568\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.73387\n",
            "Epoch 118/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0920 - val_loss: 2.8914\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.73387\n",
            "Epoch 119/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0749 - val_loss: 1.4899\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.73387\n",
            "Epoch 120/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0512 - val_loss: 4.4303\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.73387\n",
            "Epoch 121/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0499 - val_loss: 1.6785\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.73387\n",
            "Epoch 122/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0388 - val_loss: 1.6817\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.73387\n",
            "Epoch 123/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0392 - val_loss: 2.0734\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.73387\n",
            "Epoch 124/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0420 - val_loss: 1.5965\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.73387\n",
            "Epoch 125/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0611 - val_loss: 1.0410\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.73387\n",
            "Epoch 126/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0516 - val_loss: 1.1759\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.73387\n",
            "Epoch 127/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0373 - val_loss: 1.2361\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.73387\n",
            "Epoch 128/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0291 - val_loss: 1.1999\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.73387\n",
            "Epoch 129/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0208 - val_loss: 1.1851\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.73387\n",
            "Epoch 130/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0265 - val_loss: 1.4203\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.73387\n",
            "Epoch 131/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0903 - val_loss: 11.4659\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.73387\n",
            "Epoch 132/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1462 - val_loss: 9.7862\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.73387\n",
            "Epoch 133/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.1535 - val_loss: 2.5304\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.73387\n",
            "Epoch 134/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0786 - val_loss: 1.5416\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.73387\n",
            "Epoch 135/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0502 - val_loss: 0.9274\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.73387\n",
            "Epoch 136/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0312 - val_loss: 1.0699\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.73387\n",
            "Epoch 137/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0259 - val_loss: 1.0516\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.73387\n",
            "Epoch 138/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0198 - val_loss: 1.0867\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.73387\n",
            "Epoch 139/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0227 - val_loss: 1.3253\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.73387\n",
            "Epoch 140/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0240 - val_loss: 1.4027\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.73387\n",
            "Epoch 141/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0265 - val_loss: 1.1912\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.73387\n",
            "Epoch 142/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0265 - val_loss: 1.2488\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.73387\n",
            "Epoch 143/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0394 - val_loss: 1.2047\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.73387\n",
            "Epoch 144/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0502 - val_loss: 1.8683\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.73387\n",
            "Epoch 145/500\n",
            "5460/5460 [==============================] - 15s 3ms/step - loss: 0.0515 - val_loss: 1.4330\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.73387\n",
            "Epoch 146/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0431 - val_loss: 4.9231\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.73387\n",
            "Epoch 147/500\n",
            "5460/5460 [==============================] - 14s 3ms/step - loss: 0.0303 - val_loss: 1.4989\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.73387\n",
            "Epoch 148/500\n",
            " 200/5460 [>.............................] - ETA: 12s - loss: 0.0202"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewaaaS7BUMpE"
      },
      "source": [
        "a = model.evaluate(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqKwGiVVhtkq"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjhsKNRsRJtM"
      },
      "source": [
        "def sig(a):\n",
        "    if a[0] == \"-\":\n",
        "        return -1\n",
        "    else:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoa2IQaF58sh"
      },
      "source": [
        "def a(l,r):\n",
        "    cnt = 0\n",
        "    if len(l) == len(r):\n",
        "        for i in range(len(l)):\n",
        "            if l[i] != r[i]:\n",
        "                cnt = cnt + 1\n",
        "        if cnt <= 1:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zztZDSaDI8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58d0c042-f1ad-4e6a-dc00-d4f7e62fc1d7"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "act_model.load_weights('drive/My Drive/15k toloka/0385.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img)\n",
        "# print(prediction)\n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " \n",
        "# see the results\n",
        "i = 0\n",
        "true_num = 0\n",
        "false_num = 0\n",
        "for x in out:\n",
        "    asd = \"\"\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            asd = asd + char_list[int(p)]\n",
        "    if(asd == valid_orig_txt[i]):# or (valid_orig_txt[i][0]== \"-\" and  asd == valid_orig_txt[i][1:] )):\n",
        "        true_num = true_num+1\n",
        "        cv2_imshow((valid_img[i]*255).astype(int))\n",
        "        print(\"pred:\", asd ,\"true:\", valid_orig_txt[i])\n",
        "    else:\n",
        "        false_num = false_num+1\n",
        "        # cv2_imshow((valid_img[i]*255).astype(int))\n",
        "        print(\"pred:\", asd ,\"true:\", valid_orig_txt[i])\n",
        "    # print('\\n')\n",
        "    i+=1\n",
        "\n",
        "print(\"Accuracy:\", true_num/(true_num + false_num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfdY5-vRy-iB"
      },
      "source": [
        "\n",
        "# load the saved best model weights\n",
        "act_model.load_weights('best_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img)\n",
        "# print(prediction)\n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " \n",
        "# see the results\n",
        "i = 0\n",
        "j = 0\n",
        "for x in out:\n",
        "    # print(\"original_text =  \", valid_orig_txt[i])\n",
        "    # print(\"predicted text = \", end = '')\n",
        "    tr = ''\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            tr = tr + char_list[int(p)] \n",
        "            # print(char_list[int(p)], end = '')       \n",
        "    # print('\\n')\n",
        "    print(tr, valid_orig_txt[i])\n",
        "    if tr ==  valid_orig_txt[i]:\n",
        "        i+=1\n",
        "    else:\n",
        "        j+=1\n",
        "    \n",
        "\n",
        "print(i/(i+j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK7Ixom4h9n2"
      },
      "source": [
        "Plot of the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pq6Ie2v8loD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history[\"loss\"][20:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}